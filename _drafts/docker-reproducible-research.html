---
layout: post
title: Using docker for reproducible computational publications
published: false
tags: science reproducibleresearch
categories: science
date: 2014-08-21 00:11:00
---

Intro - TODO
lots of talk about reproducible research:
  cite nature series, other articles about this, my previous blog post

some papers have started publishing data/code to go with figures and
analysis (put some examples)

this is the beginning, but not there yet: need a way to make results
easily reproducible, in a way that others can interact with

outline of what I talk about
say the goal of providing all the analysis for a paper encapsulated in
a docker
say I'd love to have open discussion about this, what would be the
best way to publish paper like this

<h2>Challenges in providing easily reproducible results</h2>
challenges: want to be reproducible, but also that it is *easy* for
others to reproduce it, else it's not useful and no one will waste
their time
burden is on the author to make this easy, not just to hand the
scripts off and say good luck
will make for better code, more accountability, and research that is
more useful to other people

<h2>Using docker for reproducible analyses</h2>

say what docker is
why it can be useful for this: provide an environment where everything
is already set up. don't send me chasing some obscure libraries,
etc. can run anywhere
site: 
https://bcbio.wordpress.com/2014/03/06/improving-reproducibility-and-installation-of-genomic-analysis-pipelines-with-docker/
figshare reproducible research pdf

say how I would do it (Github repo for a paper, have Dockerfile there,
separate data directory)
makefile to get all results? several options to discuss


<h2>Example - reproducing Martin, et al. PLOS Genetics 2014</h2>
I wanted to go through an excercise of how it would actually work to
provide a Docker to reproduce analyses of a publication. As an example, I am using code and data from [Transcriptome Sequencing
from Diverse Human Populations Reveals Differentiated Regulatory
Architecture by Martin and Costa
et
al.](http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1004549). This
paper was recently published in PLOS Genetics, and the authors
published the code and data used for analysis and figures along with
the manuscript. <b>First of all, I want to give the [Bustamante
  Lab](http://med.stanford.edu/bustamantelab/) huge kudos for
  this. </b> Although many people are talking about publishing code and data, few people are
  actually providing it, and I was very happy to see this. I also want
to point out that the code I use in the example below is modified from
the code they published with the paper, and that I received permission
from the authors to post this.
<p>
Even though the data and code were provided, it was not immediate
smooth sailing: the code required many R packages that I do not have
installed. Some scripts required downloading scripts from a previous
publication. It wasn't immediately clear which scripts went with which figures. And I had to change some hard coded paths in the
scripts. But eventually I got the scripts to run with minimal
modifications, so the figures really were reproducible.
<p>
This took a fair amount of fighting and set up time to install all the things I was
missing. I can bet that almost nobody is going to patient enough to go
through this process: they'll maybe download the scripts and try
running them. But once it's not immediate runnable, they will probably
just give up. But these are exactly the problems that Docker can solve: I
can set up this compute environment once, and then provide it to other
people in such a way that it just runs, almost regardless of what
computing environment they're using.
<p>
For my example, I chose two supplemental figures from this study
(Figure S7B and figure S1) and made a Docker that provides everything
needed to reproduce those in Rstudio. I chose these mostly because they were the
first scripts I looked at that ran in under a couple minutes and that
I could clearly tell which figure the scripts belonged to. The Docker
is loaded with a running installation of [Rstudio
server](http://www.rstudio.com/products/rstudio/download-server/), and
all the code, data, and dependencies needed to produce those
figures. Each figure is in a separate script that you can open in
Rstudio and run out of the box to reproduce what you see in the
published manuscript.
<p>

<h3>Run the docker yourself</h3>

<h3>Notes</h3>

<h2>Challenges</h2>
I am no expert, but I think Docker is great, and solves a lot of
problems. That being said, there are still challenges:

<ul>
  <li>Like my labmate Assaf Gordon says, <em>"It works everywhere, except
      when it doesn't".</em>
    <p>I got this to run on my Mac (which
      because of some user errors on my part was pretty painful but
      eventually worked). But several older Ubuntu distributions wouldn't
      work, or at least weren't straightforward. This is still not a
      perfect solution that runs everywhere.</li>
  <li>How will this scale for studies working with huge datasets?
    <p>
      It's great to provide analyses that can be completely rerun by
      other researchers. But sometimes it's just not feasible for
      someone else to go rerun your analysis of 200TB of sequencing
      data that you ran across hundreds of nodes on the cloud. I don't know what the perfect reproducibility solution is for such
      huge studies, but maybe there is a partial solution. You can at
      least provide the code and computing environment that was used
      for the analyses. Ideally you would provide a subset of the data
      that another person can rerun and see that they get the same
      results. For instance, if your study processed 3,000 sequencing
      datasets, provide a Docker with the code that will allow someone
      to rerun the analysis on a single or several genomes. 
  </li>
  <li>Docker should not replace writing good code
    <p>
      One argument against the Docker solution is that you can still publish
       crappy code and distribute it in a docker. You could even
      supply just a binary file with no source code in a
       docker. But that wouldn't be very useful or in the spirit of
       reproducibility. Wouldn't a better solution be if you just made your
       code open source and able to compile and run easily on most
       reasonable systems?
      <p>
	I don't think Docker should replace the need to write good,
	clean easily runnable code. But I do think there is a tradeoff
	here between how much time a bioinformatician spends getting the
	code to run everywhere vs. how much time the user spends
	getting the time to run. The bioinformatician could spend a
	month making his or her code run smoothly everywhere, or could
	spend a day making it work once on a Docker, with a
	little bit of extra effort on the part of users to use
	Docker. But of course neither of these options should give an
	excuse to write bad code.
      <p>
	Related to this, I don't think Docker is the solution to
	everything. I think if you have some R/Python/etc. scripts
	that you used for analysis and to make figures, Docker is a
	great way to make them easily runnable by other people in such
	a way that does not require them to install a bunch of
	libraries, deal with setting up paths, etc. BUT, if you're
	publishing a tool (e.g. GATK, bwa, Bowtie, etc.), it's not
	enough to put it in a Docker and have it run. If you want
	people to heavily use your tool, it's going to have to run
	well on a variety of systems and you're going to have to put
	in that effort.
  </li>
</ul>

<h2>Conclusion</h2>
First, special thanks to Assaf Gordon and Alon Goren for all the great conversations about this.
<p>
I would love to hear what others think and discuss this and other
options for improving reproducibility. Like I mention above, I don't
think Docker is the perfect solution, or that a single perfect
solution exists, but I think it solves a lot of
problems and is a step in the right direction.
